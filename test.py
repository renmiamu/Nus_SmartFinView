import quandl
from sklearn.ensemble import RandomForestRegressor
from pypfopt.efficient_frontier import EfficientFrontier
from pypfopt import risk_models
from pypfopt import expected_returns
from pypfopt.discrete_allocation import DiscreteAllocation, get_latest_prices
import matplotlib.pyplot as plt
import base64
from io import BytesIO
from datetime import datetime, timedelta
import yfinance as yf
from fastapi import FastAPI, HTTPException, Query
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
import numpy as np
import pandas as pd
import traceback
import torch
import joblib
import tweepy
from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer
from collections import Counter
import re


# åˆå§‹åŒ–FastAPIåº”ç”¨
app = FastAPI(title="SmartFinView API", description="æ•´åˆæŠ•èµ„ç»„åˆæ¨èä¸è‚¡ç¥¨åˆ†æåŠŸèƒ½")

# é…ç½®CORS
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# é…ç½®Quandl APIï¼ˆéœ€ç”¨æˆ·è‡ªè¡Œæ³¨å†Œè·å–å¯†é’¥ï¼‰
quandl.ApiConfig.api_key = "sJpDP9EsiAb6FBCNBmUU"


# 1. æŠ•èµ„ç»„åˆæ¨èæ¥å£ç›¸å…³æ¨¡å‹ä¸å®ç°
class UserPreference(BaseModel):
    risk_tolerance: str  # "low", "medium", "high"
    industry_preference: list[str] = []  # å¦‚ ["Technology", "Healthcare"]
    investment_amount: float
    lookback_period: int = 365  # å†å²æ•°æ®å¤©æ•°


@app.post("/investment/recommendation", description="æ ¹æ®ç”¨æˆ·åå¥½ç”ŸæˆæŠ•èµ„ç»„åˆæ¨è")
def generate_recommendation(preference: UserPreference):
    try:
        # ç­›é€‰ç¬¦åˆè¡Œä¸šåå¥½çš„è‚¡ç¥¨æ± 
        industry_stocks = {
            "Technology": ["AAPL", "MSFT", "GOOGL"],
            "Healthcare": ["JNJ", "PFE", "MRNA"],
            "Finance": ["JPM", "BAC", "GS"],
            "Energy": ["XOM", "CVX", "COP"]
        }
        
        # æ ¹æ®ç”¨æˆ·åå¥½ç­›é€‰è‚¡ç¥¨
        selected_tickers = []
        for industry in preference.industry_preference:
            if industry in industry_stocks:
                selected_tickers.extend(industry_stocks[industry])
        # è‹¥æœªé€‰è¡Œä¸šï¼Œé»˜è®¤ä½¿ç”¨å¤§ç›˜è‚¡
        if not selected_tickers:
            selected_tickers = ["AAPL", "MSFT", "AMZN", "JNJ", "XOM", "JPM"]
        selected_tickers = list(set(selected_tickers))  # å»é‡

        # è·å–å†å²æ•°æ®
        end_date = datetime.now()
        start_date = end_date - timedelta(days=preference.lookback_period)
        
        print(f"å¼€å§‹è·å–å†å²æ•°æ®ï¼Œè‚¡ç¥¨: {selected_tickers}ï¼Œæ—¶é—´èŒƒå›´: {start_date} è‡³ {end_date}")
        
        # ä»Yahoo Financeè·å–ä»·æ ¼æ•°æ®
        stock_data = yf.download(
            selected_tickers, 
            start=start_date, 
            end=end_date,
            auto_adjust=False,  # æ˜¾å¼å…³é—­è‡ªåŠ¨è°ƒæ•´
            progress=False
        )
        
        print(f"è·å–åˆ°çš„è‚¡ç¥¨æ•°æ®å½¢çŠ¶: {stock_data.shape}")
        print(f"è·å–åˆ°çš„è‚¡ç¥¨æ•°æ®åˆ—å: {stock_data.columns}")
        
        # å¤„ç†Adj Closeåˆ—ï¼ˆå…¼å®¹å•åª/å¤šåªè‚¡ç¥¨çš„ç´¢å¼•ç»“æ„ï¼‰
        if isinstance(stock_data.columns, pd.MultiIndex):
            # å¤šåªè‚¡ç¥¨ï¼šåˆ—æ˜¯å¤šå±‚ç´¢å¼•ï¼ˆå¦‚(Adj Close, AAPL)ï¼‰
            if 'Adj Close' in stock_data.columns.get_level_values(0):
                stock_data = stock_data['Adj Close']  # æå–æ‰€æœ‰è‚¡ç¥¨çš„Adj Close
            else:
                # è‹¥æ²¡æœ‰Adj Closeï¼Œç”¨Closeåˆ—æ›¿ä»£
                stock_data = stock_data['Close']
                print("è­¦å‘Šï¼šæœªæ‰¾åˆ°Adj Closeåˆ—ï¼Œå·²ä½¿ç”¨Closeåˆ—æ›¿ä»£")
        else:
            # å•åªè‚¡ç¥¨ï¼šåˆ—æ˜¯å•å±‚ç´¢å¼•
            if 'Adj Close' in stock_data.columns:
                stock_data = stock_data['Adj Close']
            else:
                stock_data = stock_data['Close']
                print("è­¦å‘Šï¼šæœªæ‰¾åˆ°Adj Closeåˆ—ï¼Œå·²ä½¿ç”¨Closeåˆ—æ›¿ä»£")
        
        # è·å–S&P500æ•°æ®
        sp500_data = yf.download(
            "^GSPC", 
            start=start_date, 
            end=end_date,
            auto_adjust=False,
            progress=False
        )
        
        print(f"è·å–åˆ°çš„S&P500æ•°æ®å½¢çŠ¶: {sp500_data.shape}")
        print(f"è·å–åˆ°çš„S&P500æ•°æ®åˆ—å: {sp500_data.columns}")
        
        if isinstance(sp500_data.columns, pd.MultiIndex):
            sp500 = sp500_data['Adj Close'] if 'Adj Close' in sp500_data.columns.get_level_values(0) else sp500_data['Close']
        else:
            sp500 = sp500_data['Adj Close'] if 'Adj Close' in sp500_data.columns else sp500_data['Close']
        
        # éªŒè¯æ•°æ®æœ‰æ•ˆæ€§
        if stock_data.empty:
            raise ValueError("è·å–çš„è‚¡ç¥¨æ•°æ®ä¸ºç©ºï¼Œè¯·æ£€æŸ¥è‚¡ç¥¨ä»£ç æˆ–æ—¶é—´èŒƒå›´")
        if sp500.empty:
            raise ValueError("è·å–çš„S&P500æ•°æ®ä¸ºç©º")
        
        print(f"æœ€ç»ˆè‚¡ç¥¨æ•°æ®å½¢çŠ¶: {stock_data.shape}")
        print(f"æœ€ç»ˆS&P500æ•°æ®å½¢çŠ¶: {sp500.shape}")
        
        # è®­ç»ƒå›å½’æ¨¡å‹é¢„æµ‹é¢„æœŸæ”¶ç›Šç‡
        returns = stock_data.pct_change().dropna()
        if len(returns) < 2:
            raise ValueError("æ•°æ®ç‚¹ä¸è¶³ï¼Œæ— æ³•è®­ç»ƒæ¨¡å‹ï¼Œè¯·å¢åŠ lookback_period")
        
        X = returns.shift(1).dropna()  # å‰ä¸€å¤©æ”¶ç›Šç‡ä½œä¸ºç‰¹å¾
        y = returns.iloc[1:]  # å½“å¤©æ”¶ç›Šç‡ä½œä¸ºç›®æ ‡
        
        print(f"è®­ç»ƒæ•°æ®å½¢çŠ¶ - X: {X.shape}, y: {y.shape}")
        
        # è®­ç»ƒéšæœºæ£®æ—å›å½’æ¨¡å‹
        model = RandomForestRegressor(n_estimators=100, random_state=42)
        model.fit(X, y)
        
        # é¢„æµ‹æœªæ¥çŸ­æœŸé¢„æœŸæ”¶ç›Šç‡
        latest_returns = returns.iloc[-1:].values
        predicted_returns = model.predict(latest_returns)[0]
        expected_returns_dict = dict(zip(returns.columns, predicted_returns))
        
        # ä½¿ç”¨Markowitzä¼˜åŒ–ç®—æ³•ç”Ÿæˆæœ€ä¼˜ç»„åˆ
        mu = expected_returns.mean_historical_return(stock_data)
        S = risk_models.sample_cov(stock_data)
        
        print(f"é¢„æœŸæ”¶ç›Šç‡å½¢çŠ¶: {mu.shape}")
        print(f"åæ–¹å·®çŸ©é˜µå½¢çŠ¶: {S.shape}")
        
        # æ ¹æ®é£é™©æ‰¿å—èƒ½åŠ›è°ƒæ•´ä¼˜åŒ–ç›®æ ‡
        ef = EfficientFrontier(mu, S)
        if preference.risk_tolerance == "low":
            weights = ef.min_volatility()  # æœ€å°æ³¢åŠ¨ç‡
        elif preference.risk_tolerance == "high":
            weights = ef.max_sharpe()  # æœ€å¤§å¤æ™®æ¯”ç‡
        else:
            # ç›®æ ‡å‡½æ•°æ¥æ”¶ä¸¤ä¸ªå‚æ•°wå’Œselfï¼ˆä¼˜åŒ–å™¨å®ä¾‹ï¼‰
            weights = ef.nonconvex_objective(
                lambda w, self: -self.expected_return(w) / self.volatility(w),  # ä½¿ç”¨selfè°ƒç”¨æ–¹æ³•
                initial_guess=np.array([1/len(mu)]*len(mu))
            )
        
        cleaned_weights = ef.clean_weights()
        performance = ef.portfolio_performance(verbose=False)
        
        print(f"ä¼˜åŒ–åæƒé‡: {cleaned_weights}")
        print(f"ç»„åˆè¡¨ç°: é¢„æœŸæ”¶ç›Šç‡={performance[0]:.4f}, æ³¢åŠ¨ç‡={performance[1]:.4f}, å¤æ™®æ¯”ç‡={performance[2]:.4f}")
        
        # å›æµ‹è¡¨ç°ï¼ˆä¸S&P500å¯¹æ¯”ï¼‰
        portfolio_returns = (returns * cleaned_weights).sum(axis=1)
        cumulative_portfolio = (1 + portfolio_returns).cumprod()
        cumulative_sp500 = (1 + sp500.pct_change().dropna()).cumprod()
        
        # è®¡ç®—é£é™©æŒ‡æ ‡
        max_drawdown = (cumulative_portfolio.cummax() - cumulative_portfolio).max()
        sharpe_ratio = performance[2]
        
        print(f"æœ€å¤§å›æ’¤: {max_drawdown:.4f}, å¤æ™®æ¯”ç‡: {sharpe_ratio:.4f}")
        
        # ç¦»æ•£åˆ†é…ï¼ˆæ ¹æ®æŠ•èµ„é‡‘é¢åˆ†é…è‚¡ç¥¨æ•°é‡ï¼‰
        latest_prices = get_latest_prices(stock_data)
        da = DiscreteAllocation(cleaned_weights, latest_prices, total_portfolio_value=preference.investment_amount)
        allocation, leftover = da.lp_portfolio()
        
        print(f"ç¦»æ•£åˆ†é…ç»“æœ: {allocation}, å‰©ä½™ç°é‡‘: {leftover:.2f}")
        
        # ç»˜åˆ¶å›æµ‹å¯¹æ¯”å›¾
        plt.figure(figsize=(10, 6))
        plt.plot(cumulative_portfolio.index, cumulative_portfolio, label="Recommended Portfolio")
        plt.plot(cumulative_sp500.index, cumulative_sp500, label="S&P500")
        plt.title("Portfolio vs S&P500 Cumulative Returns")
        plt.xlabel("Date")
        plt.ylabel("Cumulative Return")
        plt.legend()
        plt.grid(True)
        
        # ä¿å­˜å›¾è¡¨ä¸ºbase64ç¼–ç ä¾›å‰ç«¯å±•ç¤º
        buf = BytesIO()
        plt.savefig(buf, format="png", bbox_inches="tight")
        buf.seek(0)
        plot_data = base64.b64encode(buf.read()).decode("utf-8")
        buf.close()
        
        print("æˆåŠŸç”Ÿæˆå›æµ‹å›¾è¡¨")
        
        return {
            "recommended_assets": [
                {"ticker": ticker, "weight": round(weight*100, 2)} 
                for ticker, weight in cleaned_weights.items() if weight > 0
            ],
            "discrete_allocation": allocation,
            "leftover_cash": round(leftover, 2),
            "performance_metrics": {
                "sharpe_ratio": round(sharpe_ratio, 2),
                "max_drawdown": round(max_drawdown, 4),
                "expected_annual_return": round(performance[0]*100, 2),
                "expected_annual_volatility": round(performance[1]*100, 2)
            },
            "backtest_chart": plot_data  # base64ç¼–ç çš„å›¾è¡¨
        }
        
    except Exception as e:
        error_trace = traceback.format_exc()
        print(f"å‘ç”Ÿå¼‚å¸¸: {str(e)}")
        print(f"å®Œæ•´å †æ ˆä¿¡æ¯: {error_trace}")
        raise HTTPException(status_code=500, detail=f"æœåŠ¡å™¨å†…éƒ¨é”™è¯¯: {str(e)}")


# 2. è‚¡ç¥¨åŸºç¡€ä¿¡æ¯æ¥å£
@app.get("/stock/basic", description="è·å–è‚¡ç¥¨å®æ—¶åŸºç¡€ä¿¡æ¯ï¼ˆä»·æ ¼ã€æˆäº¤é‡ç­‰ï¼‰")
def stock_basic(ticker: str):
    df = yf.download(ticker, period='1d', interval='1m')
    if df.empty or 'Close' not in df.columns:
        return {"error": "æœªæ‰¾åˆ°è¯¥è‚¡ç¥¨æ•°æ®"}

    latest_price = df['Close'].iloc[-1].item()
    prev_close = yf.Ticker(ticker).info['previousClose']
    change_percent = (latest_price - prev_close) / prev_close * 100
    volume = df['Volume'].iloc[-1].item()
    total_volume = df['Volume'].sum().item()

    time_series = [ts.strftime('%H:%M') for ts in df.index]
    close_series = df['Close'].to_list()  # å…¼å®¹å•åªè‚¡ç¥¨æ•°æ®ç»“æ„

    result = {
        "ticker": ticker,
        "latest_price": latest_price,
        "change_percent": change_percent,
        "volume": volume,
        "total_volume": total_volume,
        "time_series": time_series,
        "close_series": close_series,
    }
    print(result)
    return result


# 3. æ‰¹é‡æ”¶ç›Šè®¡ç®—æ¥å£ç›¸å…³æ¨¡å‹ä¸å®ç°
class Holding(BaseModel):
    ticker: str
    shares: float
    buy_price: float


@app.post("/stock/profit/batch", description="æ‰¹é‡è®¡ç®—æŒä»“æ”¶ç›Š")
def batch_stock_profit(holdings: list[Holding]):
    results = []

    for h in holdings:
        try:
            df = yf.download(h.ticker, period='1d', interval='1m', progress=False)
            if df.empty or 'Close' not in df.columns:
                raise ValueError(f"æ— æ³•è·å– {h.ticker} çš„æ•°æ®")

            current_price = df['Close'].dropna().iloc[-1]
            cost = h.shares * h.buy_price
            current_value = h.shares * current_price
            profit = current_value - cost
            return_pct = (profit / cost) * 100

            results.append({
                "ticker": h.ticker.upper(),
                "shares": h.shares,
                "buy_price": h.buy_price,
                "current_price": round(float(current_price), 2),
                "cost": round(float(cost), 2),
                "current_value": round(float(current_value), 2),
                "profit": round(float(profit), 2),
                "return_pct": round(float(return_pct), 2)
            })
        except Exception as e:
            raise HTTPException(status_code=400, detail=f"{h.ticker} æŸ¥è¯¢å¤±è´¥: {str(e)}")

    return results


# 4. è‚¡ç¥¨è¯„åˆ†æ¥å£
@app.get("/stock/score", description="åŸºäºè´¢åŠ¡æŒ‡æ ‡çš„è‚¡ç¥¨è¯„åˆ†é¢„æµ‹")
def stock_score(ticker: str = Query(..., description="è‚¡ç¥¨ä»£ç ")):
    # åŠ è½½ç‰¹å¾åã€scaler
    feature_names = [
        "marketCap", "trailingPE", "forwardPE", "priceToBook", "bookValue", "beta",
        "dividendYield", "earningsGrowth", "revenueGrowth", "totalRevenue",
        "grossMargins", "operatingMargins", "profitMargins", "returnOnAssets", "returnOnEquity"
    ]
    print("ğŸ”§ åŠ è½½ scaler ä¸­...")
    scaler = joblib.load(r"C:\software\Nus_SmartFinView\scaler.pkl")

    # å®šä¹‰æ¨¡å‹ç»“æ„
    class StockRegressor(torch.nn.Module):
        def __init__(self, input_dim):
            super().__init__()
            self.net = torch.nn.Sequential(
                torch.nn.Linear(input_dim, 64),
                torch.nn.ReLU(),
                torch.nn.Linear(64, 32),
                torch.nn.ReLU(),
                torch.nn.Linear(32, 1)
            )
        def forward(self, x):
            return self.net(x)

    try:
        info = yf.Ticker(ticker).info
        for key in feature_names[:5]:
            print(f"  - {key}: {info.get(key, None)}")

        # åˆå§‹ç‰¹å¾æå–
        features = [info.get(f, None) for f in feature_names]
        print(features)

        # åŠ è½½è®­ç»ƒé›†å¹¶è®¡ç®—å‡å€¼
        df = pd.read_csv(r"C:\software\Nus_SmartFinView\dataset\training_dataset.csv")
        feature_means = df[feature_names].mean(numeric_only=True).to_dict()

        # ç”¨å‡å€¼å¡«è¡¥ç©ºç¼ºæˆ–éæ³•å€¼
        features = [
            feature_means[f] if (v is None or not isinstance(v, (int, float)) or pd.isna(v)) else v
            for v, f in zip(features, feature_names)
        ]

        # æ ‡å‡†åŒ–
        x_scaled = scaler.transform([features])

        # è½¬æ¢ä¸º tensor
        x_tensor = torch.tensor(x_scaled, dtype=torch.float32)

        # åŠ è½½æ¨¡å‹
        model = StockRegressor(input_dim=len(feature_names))
        model.load_state_dict(torch.load(r"C:\software\Nus_SmartFinView\score_model.pt", map_location='cpu'))
        model.eval()

        # é¢„æµ‹
        with torch.no_grad():
            score = model(x_tensor).item()

        return {
            "ticker": ticker.upper(),
            "score": round(float(score), 4),
            "features": dict(zip(feature_names, features))
        }

    except Exception as e:
        print("âŒ å‡ºé”™ï¼š", e)
        raise HTTPException(status_code=400, detail=f"æ— æ³•è·å–ç‰¹å¾æˆ–æ¨¡å‹é¢„æµ‹å¤±è´¥: {str(e)}")


# 5. è‚¡ç¥¨æƒ…ç»ªåˆ†ææ¥å£
@app.get("/stock/emotion", description="åŸºäºTwitterçš„è‚¡ç¥¨æƒ…ç»ªåˆ†æ")
def stock_emotion(keyword: str):
    BEARER_TOKEN = "AAAAAAAAAAAAAAAAAAAAAITI2wEAAAAA2LljYwgcTLpwwatxzyXzYK%2F9Qos%3DsfnzN8gkDp91qqOyCEFQqCNSvuuS0RdXHUfTBcRxb6HeKcYKfe"
    client = tweepy.Client(bearer_token=BEARER_TOKEN)
    query = f"{keyword} lang:en -is:retweet"
    try:
        tweets = client.search_recent_tweets(query=query, max_results=50, tweet_fields=["text"])
        tweet_texts = [tweet.text for tweet in tweets.data] if tweets.data else []
        print(f"æŠ“å–åˆ°{len(tweet_texts)}æ¡æ¨æ–‡")
    except tweepy.TooManyRequests:
        raise HTTPException(status_code=429, detail="Twitter API è¯·æ±‚è¿‡å¤šï¼Œè¯·ç¨åå†è¯•")
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"è·å–æ¨æ–‡å¤±è´¥: {str(e)}")

    analyzer = SentimentIntensityAnalyzer()
    results = []
    all_words = []
    for text in tweet_texts:
        score = analyzer.polarity_scores(text)
        results.append(score)
        words = [w.lower() for w in re.findall(r'\b[a-zA-Z]+\b', text)]
        all_words.extend(words)
    word_freq = Counter(all_words).most_common(30)
    avg_compound = sum(r['compound'] for r in results) / len(results) if results else 0

    if avg_compound >= 0.5:
        level = "Very Positive"
        suggestion = "ğŸ”¥ æåº¦æ­£é¢æƒ…ç»ªï¼Œå¸‚åœºè¿‡çƒ­ï¼Œå»ºè®®ä¿æŒè°¨æ…"
    elif avg_compound >= 0.15:
        level = "Positive"
        suggestion = "âœ… åæ­£é¢æƒ…ç»ªï¼Œä¿¡å¿ƒå¢å¼ºï¼Œå¯é€‚å½“å…³æ³¨ä¹°å…¥æœºä¼š"
    elif avg_compound >= -0.15:
        level = "Neutral"
        suggestion = "âš–ï¸ æƒ…ç»ªä¸­æ€§ï¼Œå»ºè®®è§‚æœ›ï¼Œç­‰å¾…æ›´æ˜ç¡®ä¿¡å·"
    elif avg_compound >= -0.5:
        level = "Negative"
        suggestion = "âš ï¸ å¸‚åœºæ‚²è§‚ï¼Œå®œè°¨æ…è§‚æœ›æˆ–å°ä»“ä½è¯•æ¢"
    else:
        level = "Very Negative"
        suggestion = "â— ææ…Œæƒ…ç»ªæ˜¾è‘—ï¼Œå…³æ³¨æ½œåœ¨åè½¬æœºä¼š"
    
    return {
        "keyword": keyword,
        "tweet_count": len(tweet_texts),
        "avg_compound": round(avg_compound, 4),
        "emotion_level": level,
        "suggestion": suggestion,
        "top_words": [{"word": w, "count": c} for w, c in word_freq]
    }


if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000, reload=True)