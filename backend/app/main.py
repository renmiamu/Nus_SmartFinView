from fastapi import FastAPI, Query, HTTPException
import pandas as pd
import yfinance as yf
from pydantic import BaseModel
from fastapi.middleware.cors import CORSMiddleware
import torch
import joblib
import numpy as np
import tweepy
from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer
from collections import Counter
import re
from requests import get


app = FastAPI()
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

@app.get("/stock/basic")
def stock_basic(ticker: str):
    df = yf.download(ticker, period='1d', interval='1m')
    if df.empty or 'Close' not in df.columns:
        return {"error": "æœªæ‰¾åˆ°è¯¥è‚¡ç¥¨æ•°æ®"}

    latest_price = df['Close'].iloc[-1].item()
    prev_close = yf.Ticker(ticker).info['previousClose']
    change_percent = (latest_price - prev_close) / prev_close * 100
    volume = df['Volume'].iloc[-1].item()
    total_volume = df['Volume'].sum().item()

    time_series = [ts.strftime('%H:%M') for ts in df.index]
    close_series = df['Close'].iloc[:, 0].to_list()  # å¦‚æœæ˜¯ DataFrame

    result={
        "ticker": ticker,
        "latest_price": latest_price,
        "change_percent": change_percent,
        "volume": volume,
        "total_volume": total_volume,
        "time_series": time_series,
        "close_series": close_series,
    }
    print(result)
    return result

class Holding(BaseModel):
    ticker: str
    shares: float
    buy_price: float

@app.post("/stock/profit/batch")
def batch_stock_profit(holdings: list[Holding]):
    results = []

    for h in holdings:
        try:
            df = yf.download(h.ticker, period='1d', interval='1m', progress=False)
            if df.empty or 'Close' not in df.columns:
                raise ValueError(f"æ— æ³•è·å– {h.ticker} çš„æ•°æ®")

            current_price = df['Close'].dropna().iloc[-1]
            cost = h.shares * h.buy_price
            current_value = h.shares * current_price
            profit = current_value - cost
            return_pct = (profit / cost) * 100

            results.append({
                "ticker": h.ticker.upper(),
                "shares": h.shares,
                "buy_price": h.buy_price,
                "current_price": round(float(current_price), 2),
                "cost": round(float(cost), 2),
                "current_value": round(float(current_value), 2),
                "profit": round(float(profit), 2),
                "return_pct": round(float(return_pct), 2)
            })
        except Exception as e:
            raise HTTPException(status_code=400, detail=f"{h.ticker} æŸ¥è¯¢å¤±è´¥: {str(e)}")

    return results



@app.get("/stock/score")
def stock_score(ticker: str = Query(..., description="è‚¡ç¥¨ä»£ç ")):
    # åŠ è½½ç‰¹å¾åã€scaler
    feature_names = [
        "marketCap", "trailingPE", "forwardPE", "priceToBook", "bookValue", "beta",
        "dividendYield", "earningsGrowth", "revenueGrowth", "totalRevenue",
        "grossMargins", "operatingMargins", "profitMargins", "returnOnAssets", "returnOnEquity"
    ]
    print("ğŸ”§ åŠ è½½ scaler ä¸­...")
    scaler = joblib.load("/Users/hlshen/Desktop/Nus_SmartFinView/scaler.pkl")

    # å®šä¹‰æ¨¡å‹ç»“æ„
    class StockRegressor(torch.nn.Module):
        def __init__(self, input_dim):
            super().__init__()
            self.net = torch.nn.Sequential(
                torch.nn.Linear(input_dim, 64),
                torch.nn.ReLU(),
                torch.nn.Linear(64, 32),
                torch.nn.ReLU(),
                torch.nn.Linear(32, 1)
            )
        def forward(self, x):
            return self.net(x)

    try:
        info = yf.Ticker(ticker).info
        for key in feature_names[:5]:
            print(f"  - {key}: {info.get(key, None)}")

        # åˆå§‹ç‰¹å¾æå–
        features = [info.get(f, None) for f in feature_names]
        print(features)

        # åŠ è½½è®­ç»ƒé›†å¹¶è®¡ç®—å‡å€¼
        df = pd.read_csv("/Users/hlshen/Desktop/Nus_SmartFinView/dataset/training_dataset.csv")
        feature_means = df[feature_names].mean(numeric_only=True).to_dict()

        # ç”¨å‡å€¼å¡«è¡¥ç©ºç¼ºæˆ–éæ³•å€¼
        features = [
            feature_means[f] if (v is None or not isinstance(v, (int, float)) or pd.isna(v)) else v
            for v, f in zip(features, feature_names)
        ]

        # æ ‡å‡†åŒ–
        x_scaled = scaler.transform([features])

        # è½¬æ¢ä¸º tensor
        x_tensor = torch.tensor(x_scaled, dtype=torch.float32)

        # åŠ è½½æ¨¡å‹
        model = StockRegressor(input_dim=len(feature_names))
        model.load_state_dict(torch.load("/Users/hlshen/Desktop/Nus_SmartFinView/score_model.pt", map_location='cpu'))
        model.eval()

        # é¢„æµ‹
        with torch.no_grad():
            score = model(x_tensor).item()

        return {
            "ticker": ticker.upper(),
            "score": round(float(score), 4),
            "features": dict(zip(feature_names, features))
        }

    except Exception as e:
        print("âŒ å‡ºé”™ï¼š", e)
        raise HTTPException(status_code=400, detail=f"æ— æ³•è·å–ç‰¹å¾æˆ–æ¨¡å‹é¢„æµ‹å¤±è´¥: {str(e)}")
    

@app.get("/stock/emotion")
def stock_emotion(keyword: str):
    API_KEY = "e33e6d925ce416416e1ee5b44ce8c7b9"
    url = f"https://gnews.io/api/v4/search?q={keyword}&lang=en&max=50&token={API_KEY}"
    try:
        response = get(url)
        if response.status_code != 200:
            raise HTTPException(status_code=500, detail=f"Google News API è¯·æ±‚å¤±è´¥: {response.text}")
        data = response.json()
        articles = data.get("articles", [])
        news_texts = [article["title"] + ". " + article.get("description", "") for article in articles]
        print(f"æŠ“å–åˆ°{len(news_texts)}æ¡æ–°é—»")
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"è·å–æ–°é—»å¤±è´¥: {str(e)}")

    analyzer = SentimentIntensityAnalyzer()
    results = []
    all_words = []
    for text in news_texts:
        score = analyzer.polarity_scores(text)
        results.append(score)
        words = [w.lower() for w in text.split() if w.isalpha()]
        all_words.extend(words)
    word_freq = Counter(all_words).most_common(30)
    avg_compound = sum(r['compound'] for r in results) / len(results) if results else 0

    if avg_compound >= 0.5:
        level = "Very Positive"
        suggestion = "ğŸ”¥ æåº¦æ­£é¢æƒ…ç»ªï¼Œå¸‚åœºè¿‡çƒ­ï¼Œå»ºè®®ä¿æŒè°¨æ…"
    elif avg_compound >= 0.15:
        level = "Positive"
        suggestion = "âœ… åæ­£é¢æƒ…ç»ªï¼Œä¿¡å¿ƒå¢å¼ºï¼Œå¯é€‚å½“å…³æ³¨ä¹°å…¥æœºä¼š"
    elif avg_compound >= -0.15:
        level = "Neutral"
        suggestion = "âš–ï¸ æƒ…ç»ªä¸­æ€§ï¼Œå»ºè®®è§‚æœ›ï¼Œç­‰å¾…æ›´æ˜ç¡®ä¿¡å·"
    elif avg_compound >= -0.5:
        level = "Negative"
        suggestion = "âš ï¸ å¸‚åœºæ‚²è§‚ï¼Œå®œè°¨æ…è§‚æœ›æˆ–å°ä»“ä½è¯•æ¢"
    else:
        level = "Very Negative"
        suggestion = "â— ææ…Œæƒ…ç»ªæ˜¾è‘—ï¼Œå…³æ³¨æ½œåœ¨åè½¬æœºä¼š"
    
    return {
        "keyword": keyword,
        "news_count": len(news_texts),
        "avg_compound": round(avg_compound, 4),
        "emotion_level": level,
        "suggestion": suggestion,
        "top_words": [{"word": w, "count": c} for w, c in word_freq]
    }
