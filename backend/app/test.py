from fastapi import FastAPI, Query, HTTPException
import pandas as pd
import yfinance as yf
from pydantic import BaseModel
from fastapi.middleware.cors import CORSMiddleware
import torch
import joblib
import numpy as np
from requests import get
import quandl
from sklearn.ensemble import RandomForestRegressor
from pypfopt.efficient_frontier import EfficientFrontier
from pypfopt import risk_models
from pypfopt import expected_returns
from pypfopt.discrete_allocation import DiscreteAllocation, get_latest_prices
import matplotlib.pyplot as plt
import base64
from io import BytesIO
from datetime import datetime, timedelta
import traceback
from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer
from collections import Counter
import re


app = FastAPI()
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)


@app.get("/stock/basic")
def stock_basic(ticker: str):
    df = yf.download(ticker, period='1d', interval='1m')
    if df.empty or 'Close' not in df.columns:
        return {"error": "æœªæ‰¾åˆ°è¯¥è‚¡ç¥¨æ•°æ®"}

    latest_price = df['Close'].iloc[-1].item()
    prev_close = yf.Ticker(ticker).info.get('previousClose')
    if prev_close is None:
        return {"error": "æ— æ³•è·å–å‰æ”¶ç›˜ä»·"}
    
    change_percent = (latest_price - prev_close) / prev_close * 100
    volume = df['Volume'].iloc[-1].item()
    total_volume = df['Volume'].sum().item()

    time_series = [ts.strftime('%H:%M') for ts in df.index]
    close_series = df['Close'].tolist()

    result = {
        "ticker": ticker,
        "latest_price": latest_price,
        "change_percent": change_percent,
        "volume": volume,
        "total_volume": total_volume,
        "time_series": time_series,
        "close_series": close_series,
    }
    return result


class Holding(BaseModel):
    ticker: str
    shares: float
    buy_price: float


@app.post("/stock/profit/batch")
def batch_stock_profit(holdings: list[Holding]):
    results = []

    for h in holdings:
        try:
            df = yf.download(h.ticker, period='1d', interval='1m', progress=False)
            if df.empty or 'Close' not in df.columns:
                raise ValueError(f"æ— æ³•è·å– {h.ticker} çš„æ•°æ®")

            current_price = df['Close'].dropna().iloc[-1]
            cost = h.shares * h.buy_price
            current_value = h.shares * current_price
            profit = current_value - cost
            return_pct = (profit / cost) * 100 if cost != 0 else 0

            results.append({
                "ticker": h.ticker.upper(),
                "shares": h.shares,
                "buy_price": h.buy_price,
                "current_price": round(float(current_price), 2),
                "cost": round(float(cost), 2),
                "current_value": round(float(current_value), 2),
                "profit": round(float(profit), 2),
                "return_pct": round(float(return_pct), 2)
            })
        except Exception as e:
            raise HTTPException(status_code=400, detail=f"{h.ticker} æŸ¥è¯¢å¤±è´¥: {str(e)}")

    return results


@app.get("/stock/score")
def stock_score(ticker: str = Query(..., description="è‚¡ç¥¨ä»£ç ")):
    feature_names = [
        "marketCap", "trailingPE", "forwardPE", "priceToBook", "bookValue", "beta",
        "dividendYield", "earningsGrowth", "revenueGrowth", "totalRevenue",
        "grossMargins", "operatingMargins", "profitMargins", "returnOnAssets", "returnOnEquity"
    ]
    
    try:
        scaler = joblib.load("scaler.pkl")

        class StockRegressor(torch.nn.Module):
            def __init__(self, input_dim):
                super().__init__()
                self.net = torch.nn.Sequential(
                    torch.nn.Linear(input_dim, 64),
                    torch.nn.ReLU(),
                    torch.nn.Linear(64, 32),
                    torch.nn.ReLU(),
                    torch.nn.Linear(32, 1)
                )
            def forward(self, x):
                return self.net(x)

        info = yf.Ticker(ticker).info

        df = pd.read_csv("dataset/training_dataset.csv")
        feature_means = df[feature_names].mean(numeric_only=True).to_dict()
        features = [
            feature_means[f] if (v is None or not isinstance(v, (int, float)) or pd.isna(v)) else v
            for v, f in zip([info.get(f) for f in feature_names], feature_names)
        ]

        x_scaled = scaler.transform([features])
        x_tensor = torch.tensor(x_scaled, dtype=torch.float32)
        
        model = StockRegressor(input_dim=len(feature_names))
        model.load_state_dict(torch.load("score_model.pt", map_location='cpu'))
        model.eval()
        
        with torch.no_grad():
            score = model(x_tensor).item()

        return {
            "ticker": ticker.upper(),
            "score": round(float(score), 4),
            "features": dict(zip(feature_names, features))
        }

    except Exception as e:
        raise HTTPException(status_code=400, detail=f"æ— æ³•è·å–ç‰¹å¾æˆ–æ¨¡å‹é¢„æµ‹å¤±è´¥: {str(e)}")


@app.get("/stock/emotion")
def stock_emotion(keyword: str):
    API_KEY = "e33e6d925ce416416e1ee5b44ce8c7b9"
    url = f"https://gnews.io/api/v4/search?q={keyword}&lang=en&max=50&token={API_KEY}"
    
    try:
        response = get(url)
        if response.status_code != 200:
            raise HTTPException(status_code=500, detail=f"æ–°é—»APIè¯·æ±‚å¤±è´¥: {response.text}")
        
        articles = response.json().get("articles", [])
        
        def clean_text(text):
            return re.sub(r'[^\w\s.]', '', text).strip()
        
        news_texts = [clean_text(f"{a['title']}. {a.get('description', '')}") for a in articles]
        analyzer = SentimentIntensityAnalyzer()
        results = [analyzer.polarity_scores(text) for text in news_texts]
        
        avg_compound = sum(r['compound'] for r in results) / len(results) if results else 0
        word_freq = Counter([w.lower() for text in news_texts for w in text.split() if w.isalpha()]).most_common(30)

        if avg_compound >= 0.5:
            level, suggestion = "Very Positive", "ğŸ”¥ æåº¦æ­£é¢æƒ…ç»ªï¼Œå¸‚åœºè¿‡çƒ­ï¼Œå»ºè®®ä¿æŒè°¨æ…"
        elif avg_compound >= 0.15:
            level, suggestion = "Positive", "âœ… åæ­£é¢æƒ…ç»ªï¼Œä¿¡å¿ƒå¢å¼ºï¼Œå¯é€‚å½“å…³æ³¨ä¹°å…¥æœºä¼š"
        elif avg_compound >= -0.15:
            level, suggestion = "Neutral", "âš–ï¸ æƒ…ç»ªä¸­æ€§ï¼Œå»ºè®®è§‚æœ›ï¼Œç­‰å¾…æ›´æ˜ç¡®ä¿¡å·"
        elif avg_compound >= -0.5:
            level, suggestion = "Negative", "âš ï¸ å¸‚åœºæ‚²è§‚ï¼Œå®œè°¨æ…è§‚æœ›æˆ–å°ä»“ä½è¯•æ¢"
        else:
            level, suggestion = "Very Negative", "â— ææ…Œæƒ…ç»ªæ˜¾è‘—ï¼Œå…³æ³¨æ½œåœ¨åè½¬æœºä¼š"
        
        return {
            "keyword": keyword,
            "news_count": len(news_texts),
            "avg_compound": round(avg_compound, 4),
            "emotion_level": level,
            "suggestion": suggestion,
            "top_words": [{"word": w, "count": c} for w, c in word_freq]
        }

    except Exception as e:
        raise HTTPException(status_code=500, detail=f"è·å–æ–°é—»å¤±è´¥: {str(e)}")


class UserPreference(BaseModel):
    risk_tolerance: str  # "low", "medium", "high"
    industry_preference: list[str] = []
    investment_amount: float
    lookback_period: int = 365


from matplotlib import use as mpl_use
mpl_use('Agg')
import matplotlib.pyplot as plt


def extract_price_data(df, tickers):
    """
    æå–ä»·æ ¼æ•°æ®ï¼Œå…¼å®¹å•åª/å¤šåªè‚¡ç¥¨ï¼Œä¼˜å…ˆä½¿ç”¨Adj Closeï¼Œæ— åˆ™ç”¨Close
    tickers: è‚¡ç¥¨ä»£ç åˆ—è¡¨ï¼ˆç”¨äºå•åªè‚¡ç¥¨è½¬DataFrameæ—¶çš„åˆ—åï¼‰
    """
    if df.empty:
        raise ValueError("è·å–çš„è‚¡ç¥¨æ•°æ®ä¸ºç©º")
    
    # å¤„ç†å¤šåˆ—ï¼ˆå¤šåªè‚¡ç¥¨ï¼‰ï¼šåˆ—æ˜¯MultiIndexï¼ˆlevel0: æŒ‡æ ‡ï¼Œlevel1: è‚¡ç¥¨ä»£ç ï¼‰
    if isinstance(df.columns, pd.MultiIndex):
        available_indicators = df.columns.get_level_values(0).unique()
        target_indicator = "Adj Close" if "Adj Close" in available_indicators else "Close"
        
        price_data = df[target_indicator]
        price_data = price_data.reindex(columns=tickers).dropna(axis=1, how='all')
        
        if price_data.empty:
            raise ValueError(f"æ•°æ®ä¸­ç¼ºå°‘ {target_indicator} åˆ—")
        
        return price_data
    
    # å¤„ç†å•åˆ—ï¼ˆå•åªè‚¡ç¥¨ï¼‰ï¼šè¿”å›çš„æ˜¯Seriesï¼Œè½¬ä¸ºDataFrame
    else:
        if "Adj Close" in df.columns:
            return df[["Adj Close"]].rename(columns={"Adj Close": tickers[0]})
        elif "Close" in df.columns:
            return df[["Close"]].rename(columns={"Close": tickers[0]})
        else:
            raise ValueError("æ•°æ®ä¸­ç¼ºå°‘Adj Closeæˆ–Closeåˆ—")


@app.post("/stock/recommendation")
def generate_recommendation(preference: UserPreference):
    try:
        if preference.risk_tolerance not in ["low", "medium", "high"]:
            raise HTTPException(status_code=400, detail="é£é™©å®¹å¿åº¦å¿…é¡»ä¸º 'low'ã€'medium' æˆ– 'high'")

        industry_stocks = {
            "Technology": ["AAPL", "MSFT", "GOOGL"],
            "Healthcare": ["JNJ", "PFE", "MRNA"],
            "Finance": ["JPM", "BAC", "GS"],
            "Energy": ["XOM", "CVX", "COP"]
        }

        selected_tickers = list(set(
            [t for ind in preference.industry_preference if ind in industry_stocks for t in industry_stocks[ind]]
            or ["AAPL", "MSFT", "AMZN", "JNJ", "XOM", "JPM"]
        ))

        end_date = datetime.now()
        start_date = end_date - timedelta(days=preference.lookback_period)
        
        # æ˜¾å¼è®¾ç½®auto_adjust=Falseï¼Œç¡®ä¿æ•°æ®æ ¼å¼ç¨³å®š
        stock_data = yf.download(
            selected_tickers, 
            start=start_date, 
            end=end_date, 
            progress=False,
            auto_adjust=False
        )
        sp500_data = yf.download(
            "^GSPC", 
            start=start_date, 
            end=end_date, 
            progress=False,
            auto_adjust=False
        )

        # æå–ä»·æ ¼æ•°æ®
        stock_data = extract_price_data(stock_data, selected_tickers)
        sp500 = extract_price_data(sp500_data, ["^GSPC"])["^GSPC"]  # è½¬ä¸ºSeries

        # è°ƒè¯•ä¿¡æ¯
        print(f"è‚¡ç¥¨æ•°æ®æ ¼å¼: {type(stock_data)}, å½¢çŠ¶: {stock_data.shape}")
        print(f"è‚¡ç¥¨æ•°æ®åˆ—å: {list(stock_data.columns)}")
        print(f"æ ‡æ™®500æ•°æ®æ ¼å¼: {type(sp500)}, é•¿åº¦: {len(sp500)}")

        if stock_data.empty or sp500.empty:
            raise HTTPException(status_code=400, detail="è·å–è‚¡ç¥¨æ•°æ®ä¸ºç©ºï¼Œè¯·æ£€æŸ¥è‚¡ç¥¨ä»£ç æˆ–æ—¶é—´èŒƒå›´")

        # è®¡ç®—æ”¶ç›Šç‡
        returns = stock_data.pct_change().dropna()
        if len(returns) < 2:
            raise ValueError("æ•°æ®ä¸è¶³ï¼Œæ— æ³•ç”Ÿæˆç»„åˆ")

        # æ„å»ºæ¨¡å‹
        X = returns.shift(1).dropna()
        y = returns.iloc[1:]

        model = RandomForestRegressor(n_estimators=100, random_state=42)
        model.fit(X, y)

        latest_returns = returns.iloc[-1:].values
        if latest_returns.shape[1] != len(returns.columns):
            raise ValueError("è¾“å…¥ç»´åº¦ä¸åŒ¹é…ï¼Œå¯èƒ½ç”±äºæ•°æ®ç¼ºå¤±")

        predicted_returns = model.predict(latest_returns)[0]
        expected_returns_dict = dict(zip(returns.columns, predicted_returns))

        # æŠ•èµ„ç»„åˆä¼˜åŒ–
        mu = expected_returns.mean_historical_return(stock_data)
        S = risk_models.sample_cov(stock_data)

        ef = EfficientFrontier(mu, S)
        if preference.risk_tolerance == "low":
            weights = ef.min_volatility()
        elif preference.risk_tolerance == "high":
            weights = ef.max_sharpe()
        else:
            max_sharpe_volatility = ef.portfolio_performance()[1]  # è·å–æœ€å¤§å¤æ™®ç»„åˆçš„æ³¢åŠ¨ç‡
            weights = ef.efficient_risk(target_volatility=max_sharpe_volatility * 1.2)

        cleaned_weights = ef.clean_weights()
        if not any(cleaned_weights.values()):
            raise ValueError("æ— æ³•ç”Ÿæˆæœ‰æ•ˆæŠ•èµ„ç»„åˆï¼Œè¯·è°ƒæ•´å‚æ•°")
        
        performance = ef.portfolio_performance(verbose=False)
        latest_prices = get_latest_prices(stock_data)
        da = DiscreteAllocation(cleaned_weights, latest_prices, total_portfolio_value=preference.investment_amount)
        allocation, leftover = da.lp_portfolio()

        # å›æµ‹å›¾è¡¨
        portfolio_returns = (returns * cleaned_weights).sum(axis=1)
        cumulative_portfolio = (1 + portfolio_returns).cumprod()
        cumulative_sp500 = (1 + sp500.pct_change().dropna()).cumprod()

        plt.figure(figsize=(10, 6))
        plt.plot(cumulative_portfolio.index, cumulative_portfolio, label="æ¨èç»„åˆ")
        plt.plot(cumulative_sp500.index, cumulative_sp500, label="æ ‡æ™®500")
        plt.title("ç»„åˆä¸æ ‡æ™®500ç´¯è®¡æ”¶ç›Šå¯¹æ¯”")
        plt.xlabel("æ—¥æœŸ")
        plt.ylabel("ç´¯è®¡æ”¶ç›Š")
        plt.legend()
        plt.grid(True)
        
        buf = BytesIO()
        plt.savefig(buf, format="png", bbox_inches="tight")
        plt.close()
        buf.seek(0)
        plot_data = base64.b64encode(buf.read()).decode("utf-8")

        return {
            "recommended_assets": [{"ticker": t, "weight": round(w*100, 2)} for t, w in cleaned_weights.items() if w > 0],
            "discrete_allocation": allocation,
            "leftover_cash": round(leftover, 2),
            "performance_metrics": {
                "sharpe_ratio": round(performance[2], 2),
                "max_drawdown": round((cumulative_portfolio.cummax() - cumulative_portfolio).max(), 4),
                "expected_annual_return": round(performance[0]*100, 2),
                "expected_annual_volatility": round(performance[1]*100, 2)
            },
            "backtest_chart": plot_data
        }

    except Exception as e:
        print(f"æ¨èæ¥å£é”™è¯¯: {str(e)}")
        print(traceback.format_exc())  # æ‰“å°å®Œæ•´å †æ ˆä¿¡æ¯
        raise HTTPException(status_code=500, detail=f"æœåŠ¡å™¨å†…éƒ¨é”™è¯¯: {str(e)}")